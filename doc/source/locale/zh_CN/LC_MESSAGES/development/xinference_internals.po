# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-23 10:04+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/development/xinference_internals.rst:3
msgid "The internals of Xinference"
msgstr "Xinference的内部结构"

#: ../../source/development/xinference_internals.rst:6
msgid "Table of contents:"
msgstr "目录"

#: ../../source/development/xinference_internals.rst:9
msgid "Overview"
msgstr "概述"

#: ../../source/development/xinference_internals.rst:10
msgid ""
"Xinference leverages `Xoscar <https://github.com/xorbitsai/xoscar>`_, an "
"actor programming framework we designed, as its core component to manage "
"machines, devices, and model inference processes. Each actor serves as a "
"basic unit for model inference and various inference backends can be "
"integrate into the actor, enabling us to support multiple inference "
"engines and hardware. These actors are hosted and scheduled within actor "
"pools, which are designed to be asynchronous and non-blocking and "
"function as resource pools."
msgstr ""
"Xinference利用我们设计的actor编程框架Xoscar<https://github.com/xorbitsai/"
"xoscar>`_作为其核心组件，以管理机器、设备和模型推理进程。每个actor都是"
"模型推理的基本单元，各种推理后端可以集成到actor中，从而使我们能够支持多种"
"推理引擎和硬件。这些actor在actor池中托管和调度，actor池的设计是异步和非"
"阻塞的，具有资源池的功能\"。"

#: ../../source/development/xinference_internals.rst:22
msgid ""
"Both supervisor and worker are actor instances. Initially, an actor pool,"
" serving as a resource pool, needs to be created on each server; and each"
" actor can utilize a CPU core or a GPU device. Each server has its own "
"address (IP address or hostname), so actors on different computing nodes "
"can communicate with each other through these addresses. See `Actor`_ for"
" more information."
msgstr ""
"supervisor和worker都是actor实例。最初，需要在每台服务器上创建一个作为资源"
"池的actor池；每个actor可以使用一个 CPU 内核或 GPU 设备。每台服务器都有"
"自己的地址（IP 地址或主机名），因此不同计算节点上的角色可以通过这些地址"
"相互通信。更多信息，请参阅 `Actor`_。"

#: ../../source/development/xinference_internals.rst:27
msgid "RESTful API"
msgstr "RESTful API"

#: ../../source/development/xinference_internals.rst:28
msgid ""
"The RESTful API is implemented using `FastAPI "
"<https://github.com/tiangolo/fastapi>`_, as specified in "
"`api/restful_api.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api/restful_api.py>`_."
msgstr ""
"RESTful API是利用`FastAPI <https://github.com/tiangolo/fastapi>`_实现的，"
"具体代码在`api/restful_api.py <https://github.com/xorbitsai/inference/"
"tree/main/xinference/api/restful_api.py>`_。"

#: ../../source/development/xinference_internals.rst:35
msgid ""
"This is an example of the API ``/status``, it's corresponding function is"
" ``get_status``. You can add connection between RESTful API and the "
"backend function you want in `api/restful_api.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api/restful_api.py>`_."
msgstr ""
"这是一个 API的示例，API ``/status`` 对应函数``get_status``。您可以在`api/"
"restful_api.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/api/restful_api.py>`_中添加RESTful API和对应后端函数之间的链接"

#: ../../source/development/xinference_internals.rst:39
msgid "Command Line"
msgstr "命令行"

#: ../../source/development/xinference_internals.rst:40
msgid ""
"The Command Line is implemented using `Click "
"<https://click.palletsprojects.com/>`_, as specified in "
"`deploy/cmdline.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/deploy/cmdline.py>`_,"
" allowing users to interact with the Xinference deployment features "
"directly from the terminal."
msgstr ""
"命令行是通过`Click <https://click.palletsprojects.com/>`_实现的，具体代码"
"在`deploy/cmdline.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/deploy/cmdline.py>`_，命令行允许用户直接在终端与Xinference部署"
"的功能进行交互"

#: ../../source/development/xinference_internals.rst:45
msgid "Entry Points"
msgstr "入口点"

#: ../../source/development/xinference_internals.rst:46
msgid "Take the command-lines we implemented as examples:"
msgstr "以我们实现的命令行为例："

#: ../../source/development/xinference_internals.rst:48
msgid ""
"``xinference``: Provides commands for model management, including "
"registering/unregistering models, listing all registered/running models, "
"and launching or terminating specific models. It also features "
"interactive commands like generate and chat for testing and interacting "
"with deployed models in real-time."
msgstr ""
"``xinference``：提供命令用于模型管理，包括注册/取消注册模型、列出所有已"
"注册/运行的模型，以及启动或终止特定模型。它还提供生成语言和聊天等交互式"
"命令，用于实时测试已部署的模型并与之交互。"

#: ../../source/development/xinference_internals.rst:52
msgid ""
"``xinference-local``: Starts a local Xinference cluster for development "
"or testing purposes."
msgstr "`xinference-local``：启动用于开发或测试目的的本地Xinference集群。"

#: ../../source/development/xinference_internals.rst:54
msgid ""
"``xinference-supervisor``: Initiates a supervisor process that manages "
"and monitors worker actors within a distributed setup."
msgstr ""
"``xinference-supervisor``：启动监督进程，在分布式体系中管理和监控worker "
"actors。"

#: ../../source/development/xinference_internals.rst:56
msgid ""
"``xinference-worker``: Starts a worker process that executes tasks "
"assigned by the supervisor, utilizing available computational resources "
"effectively."
msgstr ""
"``xinference-worker``：启动worker进程，执行supervisor分配的任务，有效利用"
"可用计算资源。"

#: ../../source/development/xinference_internals.rst:59
msgid ""
"Each command is equipped with ``options`` and ``flags`` to customize its "
"behavior, such as specifying log levels, host addresses, port numbers, "
"and other relevant settings."
msgstr ""
"每条命令都配有``option``和``flag``，可自定义其行为，如指定日志级别、主机"
"地址、端口号和其他相关设置。"

#: ../../source/development/xinference_internals.rst:62
msgid ""
"Python projects define command-line console entry points in `setup.cfg` "
"or `setup.py`."
msgstr "Python项目会在`setup.cfg`或`setup.py`中定义命令行控制台入口点。"

#: ../../source/development/xinference_internals.rst:72
msgid ""
"The command-line ``xinference`` can be refered to code in "
"``xinference.deploy.cmdline:cli``."
msgstr "命令行``xinference``可参考``xinference.deploy.cmdline:cli``中的代码。"

#: ../../source/development/xinference_internals.rst:75
msgid "Click"
msgstr "Click"

#: ../../source/development/xinference_internals.rst:76
msgid "We use Click to implement a specific command-line:"
msgstr "我们使用Click来实现特定的命令行："

#: ../../source/development/xinference_internals.rst:95
msgid ""
"For example, the ``xinference-local`` command allows you to define the "
"host address and port."
msgstr "例如，`xinference-local``命令允许您定义主机地址和端口。"

#: ../../source/development/xinference_internals.rst:98
msgid "Actor"
msgstr "Actor"

#: ../../source/development/xinference_internals.rst:99
msgid ""
"Xinference is fundamentally based on `Xoscar "
"<https://github.com/xorbitsai/xoscar>`_, our actor framework, which can "
"manage computational resources and Python processes to support scalable "
"and concurrent programming. The following is a pseudocode demonstrating "
"how our Worker Actor works, the actual Worker Actor is more complex than "
"this."
msgstr ""
"Xinference以`Xoscar <https://github.com/xorbitsai/xoscar>`_为基础，Xoscar"
"是我们的actor框架，可以管理计算资源和Python进程，支持可扩展的并发编程。"
"下面的伪代码演示了Worker Actor的工作原理，实际的Worker Actor要比这个复杂"
"得多。"

#: ../../source/development/xinference_internals.rst:126
msgid ""
"We use the ``WorkerActor`` as an example to illustrate how we build the "
"Xinference. Each actor class is a standard Python class that inherits "
"from ``xoscar.Actor``. An instance of this class is a specific actor "
"within the actor pool."
msgstr ""
"我们以``WorkerActor``为例，说明如何构建Xinference。每个actor类都是继承自`"
"`xoscar.Actor``的标准Python类。该类的实例就是actor池中的一个特定的actor。"

#: ../../source/development/xinference_internals.rst:130
msgid ""
"**Define Actor Actions**: Each actor needs to define certain actions or "
"behaviors to accomplish specific tasks. For instance, the model inference"
" ``WorkerActor`` needs to launch the model (``launch_model``), list the "
"models in this actor (``list_models``), terminate a model "
"(``terminate_model``). There are two special methods worth noting. The "
"``__post_create__`` is invoked before the actor is created, allowing for "
"necessary initializations. The ``__pre_destroy__`` is called after the "
"actor is destroyed, allowing for cleanup or finalization tasks."
msgstr ""
"**定义Actor的行为**：每个actor都需要定义某些动作或行为来完成特定任务。"
"例如，模型推理``WorkerActor``需要启动模型（``launch_model``）、列出该"
"actor中的模型（``list_models``）、终止模型（``termininate_model``）。有"
"两个特殊方法值得注意。``__post_create__``在创建actor之前调用，以便进行"
"必要的初始化。而``__pre_destroy__``会在actor被销毁后调用，以便执行清理或"
"最终任务。"

#: ../../source/development/xinference_internals.rst:136
msgid ""
"**Reference Actor and Invoke Methods**: When an actor is created, it "
"yields a reference variable so that other actors can reference it. The "
"actor reference can also be referenced with the address. Suppose the "
"``WorkerActor`` is created and the reference variable is ``worker_ref``,"
"  the ``launch_model`` method of this actor class can be invoked by "
"calling ``worker_ref.launch_model()``."
msgstr ""
"**引用actor和调用方法**：当创建一个actor时，它会产生一个引用变量，以便"
"其他角色可以引用它。actor引用也可以用地址来引用。假设创建了``WorkerActor`"
"`，且引用变量为``worker_ref``，那么就可以通过调用``worker_ref.launch_"
"model()``来调用该actor类的``launch_model``。"

#: ../../source/development/xinference_internals.rst:141
msgid ""
"**Inference Engine**: The actor can manage the process, and the inference"
" engine is also a process. In the launch model part of the "
"``WorkerActor``, we can initialize different inference engines according "
"to the user's need. Therefore, Xinference can support multiple inference "
"engines and can easily adapt to new inference engines in the future."
msgstr ""
"**推理引擎**：actor可以管理进程，而推理引擎也是一进程。在``WorkerActor``"
"的启动模型部分，我们可以根据用户的需要初始化不同的推理引擎。因此，"
"Xinference可以支持多种推理引擎，并能轻松适应未来的新推理引擎。"

#: ../../source/development/xinference_internals.rst:146
msgid ""
"See `Xoscar document <https://xoscar.dev/en/latest/getting_started/llm-"
"inference.html>`_ for more actor use cases."
msgstr ""
"请参阅`Xoscar文档 <https://xoscar.dev/en/latest/getting_started/llm-"
"inference.html>`_了解更多actor用例。"

#: ../../source/development/xinference_internals.rst:149
msgid "Concurrency"
msgstr "并发性"

#: ../../source/development/xinference_internals.rst:150
msgid ""
"Both Xinference and Xoscar highly utilize coroutine programming of "
"``asyncio``."
msgstr "Xinference和Xoscar都高度利用了``asyncio``进行协程编程"

#: ../../source/development/xinference_internals.rst:152
msgid ""
"If you're not familiar with Pythons's ``asyncio``, you can see more "
"tutorials for help:"
msgstr "如果您不熟悉Python的``asyncio``，可以查看更多教程以获得帮助："

#: ../../source/development/xinference_internals.rst:154
msgid ""
"`Real Python's asyncio Tutorial <https://realpython.com/async-io-"
"python/>`__"
msgstr ""

#: ../../source/development/xinference_internals.rst:156
msgid ""
"`Python Official Documentation "
"<https://docs.python.org/3/library/asyncio.html>`__"
msgstr ""

#: ../../source/development/xinference_internals.rst:159
msgid "Model"
msgstr "模型"

#: ../../source/development/xinference_internals.rst:160
msgid ""
"Xinference supports different types of models including large language "
"models (LLMs), image models, audio models, embedding models, etc. All "
"models are implemented in `model/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model>`_. "
"Take `llm/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm>`_"
" for example, it focuses on the management and instantiation of LLMs. It "
"includes detailed implementations for loading, configuring, and deploying"
" LLMs, including handling different types of quantization and model "
"formats. In `llm/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm>`_,"
" it supports many backends such as `GGML "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/ggml>`_,"
" `PyTorch "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/pytorch>`_,"
" `SGLang "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/sglang>`_"
" and `vLLM "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/vllm>`_."
msgstr ""
"Xinference支持不同类型的模型，包括大型语言模型（LLM）、图像模型、音频模型"
"、嵌入模型等。所有模型都在`model/ <https://github.com/xorbitsai/inference"
"/tree/main/xinference/model>`_中实现。以`llm/ <https://github.com/"
"xorbitsai/inference/tree/main/xinference/model/llm>`_为例，它侧重于LLM的"
"管理和实例化。它包括加载、配置和部署LLM的详细实现，包括处理不同类型的量化"
"和模型格式。在`llm/ <https://github.com/xorbitsai/inference/tree/main/"
"xinference/model/llm>`_中，它支持许多后端，如`GGML <https://github.com/"
"xorbitsai/inference/tree/main/xinference/model/llm/ggml>`_、`PyTorch <"
"https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/"
"pytorch>`_、`SGLang <https://github.com/xorbitsai/inference/tree/main/"
"xinference/model/llm/sglang>`_和`vLLM <https://github.com/xorbitsai/"
"inference/tree/main/xinference/model/llm/vllm>`_。"

#: ../../source/development/xinference_internals.rst:171
msgid ""
"In `llm/llm_family.json "
"<https://github.com/xorbitsai/inference/blob/main/xinference/model/llm/llm_family.json>`_,"
" we utilize JSON files to manage the metadata of emerging open-source "
"models. Adding a new model does not necessitate writing new code, it "
"merely requires appending new metadata to the existing JSON file."
msgstr ""
"在`llm/llm_family.json <https://github.com/xorbitsai/inference/blob/main/"
"xinference/model/llm/llm_family.json>`_中，我们利用JSON文件来管理新出现的"
"开源模型的元数据。添加一个新模型并不需要编写新代码，只需要将新的元数据"
"添加到现有的JSON文件中即可。"

#: ../../source/development/xinference_internals.rst:198
msgid ""
"This is an example of how to define the Llama-2 chat model. The "
"``model_specs`` define the information of the model, as one model family "
"usually comes with various sizes, quantization methods, and file formats."
" For instance, the ``model_format`` could be ``pytorch`` (using Hugging "
"Face Transformers or vLLM as backend), ``ggmlv3`` (a tensor library "
"associated with llama.cpp), or ``gptq`` (a post-training quantization "
"framework). The ``model_id`` defines the repository of the model hub from"
" which Xinference downloads the checkpoint files. Furthermore, due to "
"distinct instruction-tuning processes, different model families have "
"varying prompt styles. The ``prompt_style`` in the JSON file specifies "
"how to format prompts for this particular model. For example, "
"``system_prompt`` and ``roles`` are used to specify the instructions and "
"personality of the model."
msgstr ""
"这是一个如何定义Llama-2聊天模型的示例。``model_specs``定义了模型的信息，"
"因为一个模型系列通常有不同的尺寸、量化方法和文件格式。例如，``model_"
"format``可以是``pytorch``（使用 Hugging Face Transformers或vLLM作为后端）"
"、``ggmlv3``（与 llama.cpp 相关的张量库）或``gptq``（训练后量化框架）。``"
"model_id``定义了模型中心的资源库，Xinference从模型中心下载检查点文件。"
"此外，由于不同的指令调整过程，不同的模型系列有不同的提示风格。JSON文件中"
"的``prompt_style``指定了该特定模型的提示格式。例如，``system_prompt``和``"
"roles``用于指定模型的指令和个性。"

#: ../../source/development/xinference_internals.rst:208
msgid "Code Walkthrough"
msgstr "代码指南"

#: ../../source/development/xinference_internals.rst:209
msgid ""
"The main code is located in the `xinference/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference>`_:"
msgstr ""
"主要代码位于`xinference/ <https://github.com/xorbitsai/inference/tree/"
"main/xinference>`_："

#: ../../source/development/xinference_internals.rst:211
msgid ""
"`api/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api>`_: "
"`restful_api.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api/restful_api.py>`_"
" is the core part that sets up and runs the RESTful APIs. It integrates "
"an authentication service (the specific code is located in ``oauth2/``), "
"as some or all endpoints require user authentication."
msgstr ""
"`api/ <https://github.com/xorbitsai/inference/tree/main/xinference/api>`_"
"： `restful_api.py  \"<https://github.com/xorbitsai/inference/tree/main/"
"xinference/api/restful_api.py>`_是设置和运行RESTful API的核心部分。它集成"
"了一个身份验证服务（具体代码位于\"oauth2/``），因为部分或所有端口需要用户"
"身份验证。"

#: ../../source/development/xinference_internals.rst:216
msgid ""
"`client/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/client>`_: "
"This is the client of Xinference."
msgstr ""
"`client/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"client>`_：这是Xinference的客户端。"

#: ../../source/development/xinference_internals.rst:218
msgid ""
"`oscar/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/client/oscar>`_"
" defines the Actor Client which acts as a client interface for "
"interacting with models deployed in a server environment. It includes "
"functionalities to register/unregister models, launch/terminate models, "
"and interact with different types of models. This part heavily utilizes "
"``asyncio`` for asynchronous operations. See `Concurrency`_ for more "
"information."
msgstr ""
"`oscar/  <https://github.com/xorbitsai/inference/tree/main/xinference/"
"client/oscar>`_定义了 Actor 客户端，它是一个客户端接口，用于与部署在"
"服务器环境中的模型交互。它包括注册/取消注册模型、启动/终止模型，以及与"
"不同类型的模型交互的功能。这部分主要使用``asyncio``进行异步操作。更多信息"
"，请参见`并发性`_。"

#: ../../source/development/xinference_internals.rst:223
msgid ""
"`restful/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/client/restful>`_"
" implements a RESTful client for interacting with a Xinference service."
msgstr ""
"`restful/  <https://github.com/xorbitsai/inference/tree/main/xinference/"
"client/restful>`_ 实现与Xinference服务交互的RESTful客户端。"

#: ../../source/development/xinference_internals.rst:226
msgid ""
"`core/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core>`_: "
"This is the core part of Xinference."
msgstr ""
"`core/ <https://github.com/xorbitsai/inference/tree/main/xinference/core>"
"`_：这是Xinference的核心部分。"

#: ../../source/development/xinference_internals.rst:228
msgid ""
"`metrics.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/metrics.py>`_"
" and `resource.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/resource.py>`_"
" defines a set of tools for collecting and reporting metrics and the "
"status of node resources, including model throughput, latency, the usage "
"of CPU and GPU, memory usage, and more."
msgstr ""
"`metrics.py <https://github.com/xorbitsai/inference/tree/main/xinference/"
"core/metrics.py>`_和`resource.py <https://github.com/xorbitsai/inference/"
"tree/main/xinference/core/resource.py>`_定义了一套用于收集和报告指标以及"
"节点资源状态的工具，包括模型吞吐量、延迟、CPU和GPU的使用率、内存使用率等"
"。"

#: ../../source/development/xinference_internals.rst:233
msgid ""
"`image_interface.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/image_interface.py>`_"
" and `chat_interface.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/chat_interface.py>`_"
" implement `Gradio <https://github.com/gradio-app/gradio>`_ interfaces "
"for image and chat models, respectively. These interfaces allow users to "
"interact with models through a Web UI, such as generating images or "
"engaging in chat. They build user interfaces using the gradio package and"
" communicate with backend models through our RESTful APIs."
msgstr ""
"`image_interface.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/core/image_interface.py>`_和`chat_interface.py <https://github"
".com/xorbitsai/inference/tree/main/xinference/core/chat_interface.py>`_<"
"https://github.com/gradio-app/gradio>`_分别为图像和聊天模型实现了`Gradio "
"<https://github.com/gradio-app/gradio>`_接口。这些接口允许用户通过网络"
"用户界面与模型进行交互，例如生成图像或进行聊天。代码使用gradio软件包构建"
"用户界面，并通过我们的RESTful API与后端模型通信。"

#: ../../source/development/xinference_internals.rst:239
msgid ""
"`worker.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/worker.py>`_"
" and `supervisor.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/supervisor.py>`_"
" respectively define the logic for worker actors and supervisor actor. "
"Worker actors are responsible for carrying out specific model computation"
" tasks, while supervisor actors manage the lifecycle of worker nodes, "
"schedule tasks, and monitor system states."
msgstr ""
"`worker.py <https://github.com/xorbitsai/inference/tree/main/xinference/"
"core/worker.py>`_和`supervisor.py <https://github.com/xorbitsai/inference"
"/tree/main/xinference/core/supervisor.py>`_分别定义了worker actor和"
"supervisor actor的逻辑。worker actor负责执行特定的模型计算任务，而"
"supervisor actor则管理Worker节点的生命周期和任务调度，并监控系统状态。"

#: ../../source/development/xinference_internals.rst:244
msgid ""
"`status_guard.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/status_guard.py>`_"
" implements a status monitor to track the status of models (like "
"creating, updating, terminating, etc.). It allows querying status "
"information of model instances and managing these statuses based on the "
"model's UID."
msgstr ""
"`status_guard.py  <https://github.com/xorbitsai/inference/tree/main/"
"xinference/core/status_guard.py>`_ 实现了一个状态监视器，用于跟踪模型的"
"状态（如创建、更新、终止等）。它允许根据模型的UID查询模型实例的状态信息"

#: ../../source/development/xinference_internals.rst:248
msgid ""
"`cache_tracker.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/cache_tracker.py>`_"
" defines a cache tracker for recording and managing cache status and "
"information of model versions. It supports recording cache locations and "
"statuses of model versions and querying model version information based "
"on model names."
msgstr ""
"`cache_tracker.py  <https://github.com/xorbitsai/inference/tree/main/"
"xinference/core/cache_tracker.py>`_定义了一个缓存跟踪器，用于记录和管理"
"缓存状态和模型版本信息。它支持记录缓存位置和模型版本的状态，并根据模型"
"名称查询模型版本信息。"

#: ../../source/development/xinference_internals.rst:252
#, fuzzy
msgid ""
"`event.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/event.py>`_"
" defines an event collector for gathering and reporting various runtime "
"events of models, such as information, warnings, and errors. `model.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/model.py>`_"
" defines a Model Actor, the core component for direct model interactions."
" The Model Actor is responsible for executing model inference requests, "
"handling input and output data streams, and supports various types of "
"model operations."
msgstr ""
"`event.py <https://github.com/xorbitsai/inference/tree/main/xinference/"
"core/event.py>`_定义了一个事件收集器，用于收集和报告各种运行时模型的事件"
"，如信息、警告和错误。`model.py <https://github.com/xorbitsai/inference/"
"tree/main/xinference/core/model.py>`_定义了一个模型 Actor，它是与模型直接"
"交互的核心组件。模型actor负责执行模型推理请求，处理输入和输出数据流，并"
"支持各种模型操作。这两个部分都使用`Xoscar <https://github.com/xorbitsai/"
"xoscar>`_ 用于并发和分布式执行。"

#: ../../source/development/xinference_internals.rst:258
msgid ""
"`deploy/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/deploy>`_: "
"It provides a command-line interface (CLI) for interacting with the "
"Xinference framework, allowing users to perform operations by command "
"line. See `Command Line`_ for more information."
msgstr ""
"`deploy/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"deploy>`_：它提供了一个命令行界面（CLI），用于与Xinference框架进行交互，"
"允许用户通过命令行进行操作。更多信息，请参见`命令行`_。"

#: ../../source/development/xinference_internals.rst:261
msgid ""
"`locale/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/locale>`_: "
"It supports multi-language localization. By simply adding and updating "
"JSON translation files, it becomes possible to support more languages, "
"improving user experience."
msgstr ""
"`locale/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"locale>`_：它支持多语言本地化。只需添加和更新JSON翻译文件，就可以支持更多"
"语言，改善用户体验。"

#: ../../source/development/xinference_internals.rst:264
msgid ""
"`model/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model>`_: It"
" provides a structure for model descriptions, creation, and caching. See "
"`Model`_ for more information."
msgstr ""
"`model/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"model>`_：它为模型描述、创建和缓存提供了一个框架。请参见`模型`_ 以获取更"
"多信息。"

#: ../../source/development/xinference_internals.rst:267
msgid ""
"`web/ui/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/web/ui>`_: "
"The js code of the frontend (Web UI)."
msgstr ""
"`web/ui/ <https://github.com/xorbitsai/inference/tree/main/xinference/web"
"/ui>`_：前端（用户界面）的js代码。"

#~ msgid ""
#~ "[https://realpython.com/async-io-"
#~ "python/](https://realpython.com/async-io-python/)"
#~ msgstr ""
#~ "[https://realpython.com/async-io-"
#~ "python/](https://realpython.com/async-io-python/)"

#~ msgid "[https://docs.python.org/3/library/asyncio.html](https://docs.python.org/3/library/asyncio.html)"
#~ msgstr "[https://docs.python.org/3/library/asyncio.html](https://docs.python.org/3/library/asyncio.html)"

